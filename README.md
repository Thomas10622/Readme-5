# -5-in-GameDev-# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР ЛАБОРАТОРНАЯ РАБОТА 5 [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Бархатдинов Михаил Евгеньевич
- НМТ-230803

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

Проекты Unity и скриншот TensorBoard представлены на диске: =======

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

Коэффициент корреляции — это двумерная описательная статистика, количественная мера взаимосвязи (совместной изменчивости) двух переменных. В данной лабораторной работе, коэффициентом корреляции будет максимальное расстояние, на котором агент должен оказаться от цели, в исходном скрипте это 1.42. То есть это то значение, по которому можно соотнести ожидаемый от агента результат и его итог в цикле обучения. Логирование при исходных значениях выглядит так:
![![distance3](![distance3](https://github.com/user-attachments/assets/8b349dc5-18f4-4d88-baf5-d851d1cab883)




Если уменьшить максимальную дистанцию до 1, обучение проходит немного медленнее. До 270000 шага значение среднего вознаграждения намного меньше, то есть агент в меньшем количестве случаев достигает цели. Также увеличивается среднее отклонение от цели. Ко всему прочему, если наблюдать работу обученной с таким значением модели, то будет заметно, что рядом с кубом шарик начинает кружить и выискивать нужную точку, что увеличивает время поиска цели и выглядит не очень хорошо. Скрин логирования при значении максимальной дистанции 1:
![task 1](![distance3](https://github.com/user-attachments/assets/1eb4c055-86b4-4eb4-a984-50388d738257)



Если увеличить максимальную дистанцию до 3, то значение среднего вознаграждения резко возрастает и в течение всего обучения держится очень близко к 1, иногда даже равняется 1. При этом результаты обученной модели не очень хорошие: шар может не достигнуть куба, и при этом будет считаться, что он попал в цель, хотя визуально попадание не прошло, так как объеты были отдалены друг от друга. Показатель логирования при значении максимальной дистанции 1:

![task 1](![distance3 (1)](https://github.com/user-attachments/assets/09a2a3cb-5cff-4fbd-bfb4-67d16f9d2cf3)


Подводя итоги, можно сказать, что коэффициент коррелляции в данном случае будет влиять на то, насколько сильно ML-агент будет стремиться к своей цели и находить идеальную точку попадания, минимизируя среднее отклонение от нее.

Если значение близко к 1, то шар почти в 100% случаев достигает нужной цели. Это означает, что ML-агент обучился по опыту и будет выполнять данные функции. А чем меньше значение, тем хуже агент достигает своей цели, не получая награду за это.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

В процессе работы с файлом решено изменить параметр batch_size, то есть размер одного пакета данных. Вместо 10 установлено значение 50. Это изменение в первую очередь сказалось на времени обучения, которое сократилось почти в два раза.

![batch50](https://github.com/Thomas10622/Readme-5/blob/main/1.jpg)


Вторым аспектом стало изменение количества циклов обучения с трёх до одного. Это привело к сокращению общего времени обучения и увеличению среднего значения награды. Показатели стандартного отклонения варьируются. Таким образом, количество циклов обучения может оказывать влияние на итоговую точность работы модели, но в данной ситуации видимых изменений не наблюдается, поскольку изначально модель не нуждалась в большом количестве циклов для обучения.

![epochs1](https://github.com/Thomas10622/Readme-5/blob/main/2.jpg)


Третий изменённый параметр — это максимальное число итераций в процессе обучения. Благодаря сокращению количества итераций обучение завершается быстрее, но при этом модель начинает работать медленнее. При запуске ML-агент тратит время на поиск цели, а не на движение к объекту. При таком параметре логирование происходит следующим образом:

![steps20000](https://github.com/Thomas10622/Readme-5/blob/main/3.jpg)


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

ML-агента из 1 примера можно использовать в случае, когда в игре есть постоянно перемещающийся игрок и мобы-NPC, которые пытаются его найти, например, в синглплеерном шутере (Alien Shooter), можно использовать ML-агента из первого примера. Для мобов не составит труда следовать за целью, координаты которой постоянно меняются, особенно если это происходит в 3D пространстве (или изометрическом пространстве), где есть дополнительные препятствия, которые моб не может преодолеть. В такой ситуации задача противника — не только найти игрока, но и сделать это оптимальным способом, найдя самый короткий путь к игроку из всех возможных.

ML-агент из второго примера может быть полезен, если в игре есть NPC-персонажи, которые должны выполнять монотонную работу. Это может быть актуально для игр в жанре пошаговой стратегии (Total war), особенно если в них есть элементы градостроительства и управления ресурсами (Star craft ||). В такой ситуации, если игрок меняет цель, персонажу легко перестроиться на новый путь следования, особенно с учётом постоянно меняющегося окружения.

В общем, ML-агенты следует использовать в играх, где окружение постоянно меняется. Например, когда у игрока есть возможность свободно перемещаться (большие открытые локации, открытый мир; The Witcher 3, The Long Dark). Или когда сам ландшафт локации меняется, как в играх, где игрок может строить что-то на карте (Minecraft). В таких условиях персонажу легко определить приоритеты и найти путь к игроку-цели, будь то постоянно перемещающийся игрок или какая-то конечная цель. Однако для учёта всех возможных состояний потребуется слишком большой массив переменных, что сделает написание скрипта поведения мобов/NPC вручную сложным.

## Выводы

В рамках лабораторной работы были разработаны и обучены две модели Ml-агентов, которые решают разные задачи. Первая модель должна была найти постоянно движущуюся цель, а вторая — добыть золото в руднике, перемещаясь между статичными объектами. Для модели было создано несколько вариантов с различными настройками обучения. Исследователи проследили, как результаты зависят от входных параметров.


## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
