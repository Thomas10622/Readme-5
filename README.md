# -5-in-GameDev-# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР ЛАБОРАТОРНАЯ РАБОТА 5 [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Бархатдинов Михаил Евгеньевич
- НМТ-230803

Отметка о выполнении заданий:

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

Проекты Unity и скриншот TensorBoard представлены на диске: https://drive.google.com/drive/folders/1pA91pGX9Y_NrsgUF_36_xmmAnFiXPO9E?usp=sharing

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

Коэффициент корреляции — это статистический показатель, который позволяет оценить степень взаимосвязи между двумя переменными. В данной лабораторной работе коэффициент корреляции будет представлять собой максимальное расстояние, на которое агент должен удалиться от цели. В исходном скрипте это расстояние составляет 1,42. Этот показатель позволяет оценить, насколько результат, полученный агентом, соответствует ожидаемому результату в процессе обучения. При исходных значениях логирование выглядит следующим образом:
![distance3](https://github.com/Thomas10622/Readme-5/blob/main/11.jpg)




Если сократить максимальное расстояние до 1, процесс обучения займёт больше времени. При значении среднего вознаграждения 270000 агент реже достигает цели. Также увеличивается разброс результатов. Кроме того, при использовании модели с такими настройками можно заметить, что шарик начинает кружить рядом с кубом, пытаясь найти нужную точку. Это увеличивает время поиска цели и выглядит не очень хорошо.
![distance3](https://github.com/Thomas10622/Readme-5/blob/main/22.jpg)



Если увеличить максимальное расстояние до 3, то среднее вознаграждение резко увеличивается и остаётся на уровне 1 на протяжении всего обучения, иногда даже достигая 1. Однако результаты работы обученной модели не очень удовлетворительны: шар может не достичь куба, но при этом будет считаться, что он попал в цель, хотя визуально это не будет выглядеть так, поскольку объекты находятся на значительном расстоянии друг от друга. Показатель логирования при максимальной дистанции 1:

![distance3 (1)](https://github.com/Thomas10622/Readme-5/blob/main/33.jpg)


В заключение, коэффициент корреляции в данной ситуации определяет, насколько сильно агент машинного обучения будет стремиться к своей цели и находить идеальную точку попадания, минимизируя среднее отклонение от неё.

Если значение коэффициента приближается к 1, то шар почти всегда достигает цели. Это свидетельствует о том, что агент машинного обучения успешно обучился на основе опыта и будет эффективно выполнять свои задачи. Однако, чем ниже значение коэффициента, тем хуже агент достигает своей цели, не получая за это достаточной награды.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

В процессе работы с файлом решено изменить параметр batch_size, то есть размер одного пакета данных. Вместо 10 установлено значение 50. Это изменение в первую очередь сказалось на времени обучения, которое сократилось почти в два раза.

![batch50](https://github.com/Thomas10622/Readme-5/blob/main/1.jpg)


Вторым аспектом стало изменение количества циклов обучения с трёх до одного. Это привело к сокращению общего времени обучения и увеличению среднего значения награды. Показатели стандартного отклонения варьируются. Таким образом, количество циклов обучения может оказывать влияние на итоговую точность работы модели, но в данной ситуации видимых изменений не наблюдается, поскольку изначально модель не нуждалась в большом количестве циклов для обучения.

![epochs1](https://github.com/Thomas10622/Readme-5/blob/main/2.jpg)


Третий изменённый параметр — это максимальное число итераций в процессе обучения. Благодаря сокращению количества итераций обучение завершается быстрее, но при этом модель начинает работать медленнее. При запуске ML-агент тратит время на поиск цели, а не на движение к объекту. При таком параметре логирование происходит следующим образом:

![steps20000](https://github.com/Thomas10622/Readme-5/blob/main/3.jpg)


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

ML-агента из 1 примера можно использовать в случае, когда в игре есть постоянно перемещающийся игрок и мобы-NPC, которые пытаются его найти, например, в синглплеерном шутере (Alien Shooter), можно использовать ML-агента из первого примера. Для мобов не составит труда следовать за целью, координаты которой постоянно меняются, особенно если это происходит в 3D пространстве (или изометрическом пространстве), где есть дополнительные препятствия, которые моб не может преодолеть. В такой ситуации задача противника — не только найти игрока, но и сделать это оптимальным способом, найдя самый короткий путь к игроку из всех возможных.

ML-агент из второго примера может быть полезен, если в игре есть NPC-персонажи, которые должны выполнять монотонную работу. Это может быть актуально для игр в жанре пошаговой стратегии (Total war), особенно если в них есть элементы градостроительства и управления ресурсами (Star craft ||). В такой ситуации, если игрок меняет цель, персонажу легко перестроиться на новый путь следования, особенно с учётом постоянно меняющегося окружения.

В общем, ML-агенты следует использовать в играх, где окружение постоянно меняется. Например, когда у игрока есть возможность свободно перемещаться (большие открытые локации, открытый мир; The Witcher 3, The Long Dark). Или когда сам ландшафт локации меняется, как в играх, где игрок может строить что-то на карте (Minecraft). В таких условиях персонажу легко определить приоритеты и найти путь к игроку-цели, будь то постоянно перемещающийся игрок или какая-то конечная цель. Однако для учёта всех возможных состояний потребуется слишком большой массив переменных, что сделает написание скрипта поведения мобов/NPC вручную сложным.

## Выводы

В рамках лабораторной работы были разработаны и обучены две модели Ml-агентов, которые решают разные задачи. Первая модель должна была найти постоянно движущуюся цель, а вторая — добыть золото в руднике, перемещаясь между статичными объектами. Для модели было создано несколько вариантов с различными настройками обучения. Исследователи проследили, как результаты зависят от входных параметров.


## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
